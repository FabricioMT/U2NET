{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabri\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "train images:  3633\n",
      "train labels:  3633\n",
      "---\n",
      "---define optimizer...\n",
      "---start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabri\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3722: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "c:\\Users\\fabri\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:   1/  2, batch:     6/ 3633, ite: 1] train loss: 0.154288, tar: 0.014283 \n",
      "[epoch:   1/  2, batch:    12/ 3633, ite: 2] train loss: 0.150044, tar: 0.013421 \n",
      "[epoch:   1/  2, batch:    18/ 3633, ite: 3] train loss: 0.145849, tar: 0.012907 \n",
      "[epoch:   1/  2, batch:    24/ 3633, ite: 4] train loss: 0.141385, tar: 0.012318 \n",
      "[epoch:   1/  2, batch:    30/ 3633, ite: 5] train loss: 0.139259, tar: 0.012203 \n",
      "[epoch:   1/  2, batch:    36/ 3633, ite: 6] train loss: 0.134229, tar: 0.011575 \n",
      "[epoch:   1/  2, batch:    42/ 3633, ite: 7] train loss: 0.135354, tar: 0.011698 \n",
      "[epoch:   1/  2, batch:    48/ 3633, ite: 8] train loss: 0.135148, tar: 0.011695 \n",
      "[epoch:   1/  2, batch:    54/ 3633, ite: 9] train loss: 0.132052, tar: 0.011409 \n",
      "[epoch:   1/  2, batch:    60/ 3633, ite: 10] train loss: 0.129512, tar: 0.011228 \n",
      "[epoch:   1/  2, batch:    66/ 3633, ite: 11] train loss: 0.134741, tar: 0.011496 \n",
      "[epoch:   1/  2, batch:    72/ 3633, ite: 12] train loss: 0.145976, tar: 0.012482 \n",
      "[epoch:   1/  2, batch:    78/ 3633, ite: 13] train loss: 0.137395, tar: 0.011850 \n",
      "[epoch:   1/  2, batch:    84/ 3633, ite: 14] train loss: 0.127633, tar: 0.011012 \n",
      "[epoch:   1/  2, batch:    90/ 3633, ite: 15] train loss: 0.127079, tar: 0.010926 \n",
      "[epoch:   1/  2, batch:    96/ 3633, ite: 16] train loss: 0.125894, tar: 0.010861 \n",
      "[epoch:   1/  2, batch:   102/ 3633, ite: 17] train loss: 0.130788, tar: 0.011706 \n",
      "[epoch:   1/  2, batch:   108/ 3633, ite: 18] train loss: 0.133824, tar: 0.011823 \n",
      "[epoch:   1/  2, batch:   114/ 3633, ite: 19] train loss: 0.130024, tar: 0.011491 \n",
      "[epoch:   1/  2, batch:   120/ 3633, ite: 20] train loss: 0.128254, tar: 0.011295 \n",
      "[epoch:   1/  2, batch:   126/ 3633, ite: 21] train loss: 0.131563, tar: 0.011266 \n",
      "[epoch:   1/  2, batch:   132/ 3633, ite: 22] train loss: 0.175557, tar: 0.015654 \n",
      "[epoch:   1/  2, batch:   138/ 3633, ite: 23] train loss: 0.163633, tar: 0.014313 \n",
      "[epoch:   1/  2, batch:   144/ 3633, ite: 24] train loss: 0.159213, tar: 0.014089 \n",
      "[epoch:   1/  2, batch:   150/ 3633, ite: 25] train loss: 0.156028, tar: 0.014034 \n",
      "[epoch:   1/  2, batch:   156/ 3633, ite: 26] train loss: 0.156276, tar: 0.013806 \n",
      "[epoch:   1/  2, batch:   162/ 3633, ite: 27] train loss: 0.152575, tar: 0.013585 \n",
      "[epoch:   1/  2, batch:   168/ 3633, ite: 28] train loss: 0.150505, tar: 0.013301 \n",
      "[epoch:   1/  2, batch:   174/ 3633, ite: 29] train loss: 0.148279, tar: 0.013050 \n",
      "[epoch:   1/  2, batch:   180/ 3633, ite: 30] train loss: 0.148396, tar: 0.013009 \n",
      "[epoch:   1/  2, batch:   186/ 3633, ite: 31] train loss: 0.145525, tar: 0.013540 \n",
      "[epoch:   1/  2, batch:   192/ 3633, ite: 32] train loss: 0.145950, tar: 0.013995 \n",
      "[epoch:   1/  2, batch:   198/ 3633, ite: 33] train loss: 0.143131, tar: 0.013802 \n",
      "[epoch:   1/  2, batch:   204/ 3633, ite: 34] train loss: 0.145984, tar: 0.013491 \n",
      "[epoch:   1/  2, batch:   210/ 3633, ite: 35] train loss: 0.147983, tar: 0.013576 \n",
      "[epoch:   1/  2, batch:   216/ 3633, ite: 36] train loss: 0.142885, tar: 0.012990 \n",
      "[epoch:   1/  2, batch:   222/ 3633, ite: 37] train loss: 0.142674, tar: 0.013061 \n",
      "[epoch:   1/  2, batch:   228/ 3633, ite: 38] train loss: 0.141907, tar: 0.013024 \n",
      "[epoch:   1/  2, batch:   234/ 3633, ite: 39] train loss: 0.141419, tar: 0.013025 \n",
      "[epoch:   1/  2, batch:   240/ 3633, ite: 40] train loss: 0.141259, tar: 0.013072 \n",
      "[epoch:   1/  2, batch:   246/ 3633, ite: 41] train loss: 0.115543, tar: 0.010481 \n",
      "[epoch:   1/  2, batch:   252/ 3633, ite: 42] train loss: 0.126523, tar: 0.011920 \n",
      "[epoch:   1/  2, batch:   258/ 3633, ite: 43] train loss: 0.139791, tar: 0.012716 \n",
      "[epoch:   1/  2, batch:   264/ 3633, ite: 44] train loss: 0.137471, tar: 0.012546 \n",
      "[epoch:   1/  2, batch:   270/ 3633, ite: 45] train loss: 0.135373, tar: 0.012079 \n",
      "[epoch:   1/  2, batch:   276/ 3633, ite: 46] train loss: 0.138364, tar: 0.012323 \n",
      "[epoch:   1/  2, batch:   282/ 3633, ite: 47] train loss: 0.141392, tar: 0.012470 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fabri\\OneDrive\\Área de Trabalho\\Py Script\\U-2-Net\\train.ipynb Célula: 1\u001b[0m in \u001b[0;36m<cell line: 124>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fabri/OneDrive/%C3%81rea%20de%20Trabalho/Py%20Script/U-2-Net/train.ipynb#W0sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m d0, d1, d2, d3, d4, d5, d6 \u001b[39m=\u001b[39m net(inputs_v)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fabri/OneDrive/%C3%81rea%20de%20Trabalho/Py%20Script/U-2-Net/train.ipynb#W0sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m loss2, loss \u001b[39m=\u001b[39m muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/fabri/OneDrive/%C3%81rea%20de%20Trabalho/Py%20Script/U-2-Net/train.ipynb#W0sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fabri/OneDrive/%C3%81rea%20de%20Trabalho/Py%20Script/U-2-Net/train.ipynb#W0sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fabri/OneDrive/%C3%81rea%20de%20Trabalho/Py%20Script/U-2-Net/train.ipynb#W0sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m \u001b[39m# # print statistics\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fabri\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\fabri\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as standard_transforms\n",
    "\n",
    "from data_loader import Rescale\n",
    "from data_loader import RescaleT\n",
    "from data_loader import RandomCrop\n",
    "from data_loader import ToTensor\n",
    "from data_loader import ToTensorLab\n",
    "from data_loader import SalObjDataset\n",
    "\n",
    "from model import U2NET\n",
    "from model import U2NETP\n",
    "\n",
    "# ------- 1. define loss function --------\n",
    "\n",
    "bce_loss = nn.BCELoss(size_average=True)\n",
    "\n",
    "def muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v):\n",
    "\n",
    "\tloss0 = bce_loss(d0,labels_v)\n",
    "\tloss1 = bce_loss(d1,labels_v)\n",
    "\tloss2 = bce_loss(d2,labels_v)\n",
    "\tloss3 = bce_loss(d3,labels_v)\n",
    "\tloss4 = bce_loss(d4,labels_v)\n",
    "\tloss5 = bce_loss(d5,labels_v)\n",
    "\tloss6 = bce_loss(d6,labels_v)\n",
    "\n",
    "\tloss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6\n",
    "\t#print(\"l0: %3f, l1: %3f, l2: %3f, l3: %3f, l4: %3f, l5: %3f, l6: %3f\\n\"%(loss0.data[0],loss1.data[0],loss2.data[0],loss3.data[0],loss4.data[0],loss5.data[0],loss6.data[0]))\n",
    "\n",
    "\treturn loss0, loss\n",
    "\n",
    "def plot_losses(losses):\n",
    "  fig = plt.figure(figsize=(13, 5))\n",
    "  ax = fig.gca()\n",
    "  for loss_name, loss_values in losses.items():  \n",
    "    ax.plot(loss_values, label=loss_name)\n",
    "  ax.legend(fontsize=\"16\")\n",
    "  ax.set_xlabel(\"Iteration\", fontsize=\"16\")\n",
    "  ax.set_ylabel(\"Loss\", fontsize=\"16\")\n",
    "  ax.set_title(\"Loss vs iterations\", fontsize=\"16\")\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "# ------- 2. set the directory of training dataset --------\n",
    "model_name = 'u2net' #'u2netp'\n",
    "\n",
    "data_dir = os.path.join('D:' + os.sep ,'data_train' + os.sep)\n",
    "tra_image_dir = os.path.join('image' + os.sep)\n",
    "tra_label_dir = os.path.join('mask' + os.sep)\n",
    "\n",
    "image_ext = '.png'\n",
    "label_ext = '.png'\n",
    "\n",
    "save_dir = './model_saved/'\n",
    "epoch_num = 2\n",
    "batch_size_train = 6\n",
    "batch_size_val = 1\n",
    "train_num = 0\n",
    "val_num = 0\n",
    "learning_rate = 1e-8\n",
    "save_frq = 50 # save the model every 2000 iterations\n",
    "\n",
    "tra_img_name_list = glob.glob(data_dir + tra_image_dir + '*' + image_ext)\n",
    "\n",
    "tra_lbl_name_list = []\n",
    "for img_path in tra_img_name_list:\n",
    "\timg_name = img_path.split(os.sep)[-1]\n",
    "\n",
    "\taaa = img_name.split(\".\")\n",
    "\tbbb = aaa[0:-1]\n",
    "\timidx = bbb[0]\n",
    "\tfor i in range(1,len(bbb)):\n",
    "\t\timidx = imidx + \".\" + bbb[i]\n",
    "\n",
    "\ttra_lbl_name_list.append(data_dir + tra_label_dir + imidx + label_ext)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"train images: \", len(tra_img_name_list))\n",
    "print(\"train labels: \", len(tra_lbl_name_list))\n",
    "print(\"---\")\n",
    "\n",
    "train_num = len(tra_img_name_list)\n",
    "\n",
    "salobj_dataset = SalObjDataset(\n",
    "    img_name_list=tra_img_name_list,\n",
    "    lbl_name_list=tra_lbl_name_list,\n",
    "    transform=transforms.Compose([\n",
    "        RescaleT(320),\n",
    "        RandomCrop(288),\n",
    "        ToTensorLab(flag=0)]))\n",
    "salobj_dataloader = DataLoader(salobj_dataset, batch_size=batch_size_train, shuffle=True, num_workers=1)\n",
    "\n",
    "# ------- 3. define model --------\n",
    "# define the net\n",
    "net = U2NET(3, 1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net.load_state_dict(torch.load(save_dir + \"u2net.pth\"))\n",
    "    net.cuda()\n",
    "\n",
    "# ------- 4. define optimizer --------\n",
    "print(\"---define optimizer...\")\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "\n",
    "# ------- 5. training process --------\n",
    "print(\"---start training...\")\n",
    "ite_num = 0\n",
    "running_loss = 0.0\n",
    "running_tar_loss = 0.0\n",
    "ite_num4val = 0\n",
    "\n",
    "for epoch in range(0, epoch_num):\n",
    "    net.train()\n",
    "\n",
    "    for i, data in enumerate(salobj_dataloader):\n",
    "        ite_num = ite_num + 1\n",
    "        ite_num4val = ite_num4val + 1\n",
    "\n",
    "        inputs, labels = data['image'], data['label']\n",
    "\n",
    "        inputs = inputs.type(torch.FloatTensor)\n",
    "        labels = labels.type(torch.FloatTensor)\n",
    "\n",
    "        # wrap them in Variable\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_v, labels_v = Variable(inputs.cuda(), requires_grad=False), Variable(labels.cuda(),\n",
    "                                                                                        requires_grad=False)\n",
    "        else:\n",
    "            inputs_v, labels_v = Variable(inputs, requires_grad=False), Variable(labels, requires_grad=False)\n",
    "\n",
    "        # y zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        d0, d1, d2, d3, d4, d5, d6 = net(inputs_v)\n",
    "        loss2, loss = muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # # print statistics\n",
    "        running_loss += loss.data.item()\n",
    "        running_tar_loss += loss2.data.item()\n",
    "        losses = {\"Train loss\": running_loss / ite_num4val, \"Tar loss\": running_tar_loss / ite_num4val}\n",
    "\n",
    "        # del temporary outputs and loss\n",
    "        del d0, d1, d2, d3, d4, d5, d6, loss2, loss\n",
    "\n",
    "        print(\"[epoch: %3d/%3d, batch: %5d/%5d, ite: %d] train loss: %3f, tar: %3f \" % (\n",
    "        epoch + 1, epoch_num, (i + 1) * batch_size_train, train_num, ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n",
    "\n",
    "        if ite_num % save_frq == 0:\n",
    "            #plot_losses(losses)\n",
    "            torch.save(net.state_dict(), save_dir + model_name+\"_bce_itr_%d_train_%3f_tar_%3f.pth\" % (ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n",
    "            running_loss = 0.0\n",
    "            running_tar_loss = 0.0\n",
    "            net.train()  # resume train\n",
    "            ite_num4val = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 1660 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2102, -1.1713]])\n",
      "tensor([[ 1.1565, -1.1702]], device='cuda:0')\n",
      "tensor([[ 0.2102, -1.1713]])\n",
      "False\n",
      "tensor([[ 0.2102, -1.1713]], device='cuda:0')\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "t1 = torch.randn(1,2)\n",
    "t2 = torch.randn(1,2).to(dev)\n",
    "print(t1)  # tensor([[-0.2678,  1.9252]])\n",
    "print(t2)  # tensor([[ 0.5117, -3.6247]], device='cuda:0')\n",
    "t1.to(dev)\n",
    "print(t1)  # tensor([[-0.2678,  1.9252]])\n",
    "print(t1.is_cuda) # False\n",
    "t1 = t1.to(dev)\n",
    "print(t1)  # tensor([[-0.2678,  1.9252]], device='cuda:0')\n",
    "print(t1.is_cuda) # True\n",
    "\n",
    "class M(nn.Module):\n",
    "    def __init__(self):        \n",
    "        super().__init__()        \n",
    "        self.l1 = nn.Linear(1,2)\n",
    "\n",
    "    def forward(self, x):                      \n",
    "        x = self.l1(x)\n",
    "        return x\n",
    "model = M()   # not on cuda\n",
    "model.to(dev) # is on cuda (all parameters)\n",
    "print(next(model.parameters()).is_cuda) # True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6d67bf77025d748772b843bcacfdcb5a8247f14740a6d44b4b3ceee657fb5e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
